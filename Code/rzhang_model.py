import keras
import keras.backend as K

from keras.models import Sequential
from keras.layers import Convolution2D, ZeroPadding2D
from keras.layers.normalization import BatchNormalization
from keras.layers import Activation, Flatten, Dense, Reshape

from keras.activations import softmax
from keras.optimizers import Adam

import functools
softmax3 = functools.partial(softmax, axis = 3)
softmax3.__name__ = 'softmax3'

AB_BIN_WEIGHTS = [0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022683875960096394, 0.0022701514488164282, 0.0022701514488164282, 0.0022692691796621298, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022639899323660677, 0.002260484059354882, 0.002266626482016857, 0.002267506697060341, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.002267506697060341, 0.002241394204601768, 0.0022336773300481384, 0.002264868100464008, 0.0022319696796154315, 0.0022091693402192733, 0.00215137779747977, 0.0021529643109304136, 0.002226862358492229, 0.0022578617718606478, 0.0022692691796621298, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022613595092805684, 0.0022217783577366725, 0.0021876494360075008, 0.0022008425383961473, 0.002119361883231732, 0.001908124001954105, 0.0015624533084842031, 0.0015641274780188452, 0.0018263578991269066, 0.0021124611282973037, 0.0022422549277471057, 0.002267506697060341, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022692691796621298, 0.0021721866155591757, 0.0018738325673884526, 0.0015738240091854817, 0.0016545424775093061, 0.0016419801578567428, 0.001260559793541178, 0.0007593906077023949, 0.00060332375291495, 0.000819156768135748, 0.0014648509985228488, 0.002009239879317096, 0.0022200888455365334, 0.0022613595092805684, 0.0022683875960096394, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0020581061158842207, 0.0013661451187699577, 0.0007553628351225719, 0.0005590741993200676, 0.0004547491868847851, 0.000398049236181538, 0.00023146745471483223, 0.00021732977932181793, 0.0003341530002419387, 0.0007380806653742855, 0.001466322440609064, 0.001975255039905242, 0.0021466322553506103, 0.0022311168331513747, 0.0022692691796621298, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0021577379219253423, 0.0010867164211620312, 0.00030931777182422273, 0.00013504651065775655, 7.518975659822808e-05, 3.878197354580426e-05, 2.755266024810407e-05, 8.05747510329907e-05, 0.00017891292915754315, 0.000450782049522368, 0.001029794009741643, 0.0016953766052291062, 0.0020823270502972244, 0.0022016723928287964, 0.002250031286774244, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0021958765192853637, 0.0012835592576171864, 0.00048152542855748855, 0.00027423419491804524, 0.0002145507775384476, 0.000136431746035496, 4.981242983661773e-05, 5.209962021900112e-05, 8.733972423518476e-05, 0.00019081080267606437, 0.0005185129464406754, 0.0011487157597858174, 0.0017513114594162626, 0.002149002406578356, 0.0022302646381919342, 0.0022578617718606478, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.002264868100464008, 0.0018877662878872517, 0.0011207148792924501, 0.0009062458863609847, 0.0010600816144021954, 0.001260831989027692, 0.0013444407164114804, 0.0010686180136602764, 0.0006063309483055392, 0.00038485271132143874, 0.00039282727913843367, 0.0007050179758789724, 0.00126740012988185, 0.0018183951608801811, 0.002094276923041832, 0.0021934019043175863, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.002103329723802333, 0.0018483268084508954, 0.0016948844902988963, 0.001746596708149173, 0.002010623613959107, 0.002174613565008838, 0.0021239874885492873, 0.002059558000272694, 0.0017682831626510353, 0.0012771022739579163, 0.0009620922113074756, 0.0010709700352637198, 0.0015126663545154825, 0.0019462854307141576, 0.002174613565008838, 0.0022474331808693838, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.002245704441065276, 0.0021078855367371156, 0.0020705127749715627, 0.002115522574493357, 0.002217559389596289, 0.0022526354066213913, 0.0022561169575854565, 0.002241394204601768, 0.0022234704433702164, 0.0021754237537596802, 0.0020508772912494057, 0.0017976812110477038, 0.0015763733432476706, 0.0016355411690438131, 0.001975923466232843, 0.0021967026322445384, 0.002260484059354882, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022294130939908502, 0.0022158762853535306, 0.0022396747391991503, 0.0022631124449967854, 0.0022683875960096394, 0.0022639899323660677, 0.0022561169575854565, 0.0022631124449967854, 0.0022543748379181446, 0.002244841068097983, 0.002177045943708313, 0.002164938161538321, 0.002102572335566923, 0.0021147563813558173, 0.002093526039260358, 0.0022091693402192733, 0.0022613595092805684, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.002264868100464008, 0.0022596092870003468, 0.002266626482016857, 0.0022710344042723954, 0.0022692691796621298, 0.002266626482016857, 0.002264868100464008, 0.002262235637564948, 0.002266626482016857, 0.0022657469500830473, 0.0022543748379181446, 0.0022158762853535306, 0.0022074989430052538, 0.0022192450526400736, 0.002235387595482995, 0.002258735191430638, 0.0022692691796621298, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022692691796621298, 0.0022692691796621298, 0.0022701514488164282, 0.0022710344042723954, 0.0022692691796621298, 0.0022683875960096394, 0.002267506697060341, 0.0022639899323660677, 0.002245704441065276, 0.0022474331808693838, 0.0022422549277471057, 0.0022657469500830473, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022692691796621298, 0.0022657469500830473, 0.0022692691796621298, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022657469500830473, 0.0022613595092805684, 0.0022657469500830473, 0.002262235637564948, 0.0022422549277471057, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022692691796621298, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954]
RZHANG_LOSS_WEIGHT = 1000

def custom_loss(y_true, y_pred):
    y_pred_clipped = K.clip(y_pred, K.epsilon(), None)
    return -K.sum(y_true * K.log(y_pred_clipped))


def custom_loss_rebalancing(y_true, y_pred):
    y_pred_clipped = K.clip(y_pred, K.epsilon(), None)

    num_images = y_true.shape[0]
    img_height = y_true.shape[1]
    img_width = y_true.shape[2]

    loss = -K.sum(y_true * K.log(y_pred_clipped), axis = 3)

    bin_indices = K.argmax(y_true, axis = 3)
    weights = K.gather(reference = AB_BIN_WEIGHTS, indices = bin_indices)

    loss = RZHANG_LOSS_WEIGHT * K.sum(loss * weights)

    return loss

def custom_loss_rebalancing_segments(y_true, y_pred):
    y_pred_clipped = K.clip(y_pred, K.epsilon(), None)

    img_height = y_true.shape[1]
    img_width = y_true.shape[2]

    loss = -K.sum(y_true * K.log(y_pred_clipped), axis = 3)

    bin_indices = K.argmax(y_true, axis = 3)
    weights = K.gather(reference = AB_BIN_WEIGHTS, indices = bin_indices)

    loss = RZHANG_LOSS_WEIGHT * K.sum(loss * weights)

    loss += K.sum((y_pred[:, :-1, :, :] - y_pred[:, 1:, :, :]) ** 2)
    loss += K.sum((y_pred[:, :, :-1, :] - y_pred[:, :, 1:, :]) ** 2)

    return loss



# TODO: upgrade keras using:
# sudo pip install git+git://github.com/fchollet/keras.git --upgrade


def build_model():
    model = Sequential()

    #### conv1
    # conv1_1
    model.add(ZeroPadding2D(padding = (1, 1), input_shape = (224, 224, 1)))
    model.add(Convolution2D(64, (3, 3)))
    model.add(Activation('relu'))

    # conv1_2
    model.add(ZeroPadding2D(padding = (1, 1)))
    model.add(Convolution2D(64, (3, 3), strides = (2, 2)))
    model.add(Activation('relu'))

    # conv1_2norm
    model.add(BatchNormalization())


    #### conv2
    # conv2_1
    model.add(ZeroPadding2D(padding = (1, 1)))
    model.add(Convolution2D(128, (3, 3)))
    model.add(Activation('relu'))

    # conv2_2
    model.add(ZeroPadding2D(padding = (1, 1)))
    model.add(Convolution2D(128, (3, 3), strides = (2, 2)))
    model.add(Activation('relu'))

    # conv2_2norm
    model.add(BatchNormalization())


    #### conv3
    # conv3_1
    model.add(ZeroPadding2D(padding = (1, 1)))
    model.add(Convolution2D(256, (3, 3)))
    model.add(Activation('relu'))

    # conv3_2
    model.add(ZeroPadding2D(padding = (1, 1)))
    model.add(Convolution2D(256, (3, 3)))
    model.add(Activation('relu'))

    # conv3_3
    model.add(ZeroPadding2D(padding = (1, 1)))
    model.add(Convolution2D(256, (3, 3), strides = (2, 2)))
    model.add(Activation('relu'))

    # conv3_3norm
    model.add(BatchNormalization())


    #### conv4
    # conv4_1
    model.add(ZeroPadding2D(padding = (1, 1)))
    model.add(Convolution2D(512, (3, 3)))
    model.add(Activation('relu'))

    # conv4_2
    model.add(ZeroPadding2D(padding = (1, 1)))
    model.add(Convolution2D(512, (3, 3)))
    model.add(Activation('relu'))

    # conv4_3
    model.add(ZeroPadding2D(padding = (1, 1)))
    model.add(Convolution2D(512, (3, 3)))
    model.add(Activation('relu'))

    #conv4_3norm
    model.add(BatchNormalization())


    #### conv5
    # conv5_1
    model.add(ZeroPadding2D(padding = (2, 2)))
    model.add(Convolution2D(512, (3, 3), dilation_rate = (2, 2)))
    model.add(Activation('relu'))

    # conv5_2
    model.add(ZeroPadding2D(padding = (2, 2)))
    model.add(Convolution2D(512, (3, 3), dilation_rate = (2, 2)))
    model.add(Activation('relu'))

    # conv5_3
    model.add(ZeroPadding2D(padding = (2, 2)))
    model.add(Convolution2D(512, (3, 3), dilation_rate = (2, 2)))
    model.add(Activation('relu'))

    # conv5_3norm
    model.add(BatchNormalization())


    #### conv6
    # conv6_1
    model.add(ZeroPadding2D(padding = (2, 2)))
    model.add(Convolution2D(512, (3, 3), dilation_rate = (2, 2)))
    model.add(Activation('relu'))

    # conv6_2
    model.add(ZeroPadding2D(padding = (2, 2)))
    model.add(Convolution2D(512, (3, 3), dilation_rate = (2, 2)))
    model.add(Activation('relu'))

    # conv6_3
    model.add(ZeroPadding2D(padding = (2, 2)))
    model.add(Convolution2D(512, (3, 3), dilation_rate = (2, 2)))
    model.add(Activation('relu'))

    # conv6_3norm
    model.add(BatchNormalization())


    #### conv7
    # conv7_1
    model.add(ZeroPadding2D(padding = (1, 1)))
    model.add(Convolution2D(512, (3, 3)))
    model.add(Activation('relu'))

    # conv7_2
    model.add(ZeroPadding2D(padding = (1, 1)))
    model.add(Convolution2D(512, (3, 3)))
    model.add(Activation('relu'))

    # conv7_3
    model.add(ZeroPadding2D(padding = (1, 1)))
    model.add(Convolution2D(512, (3, 3)))
    model.add(Activation('relu'))

    # conv7_3norm
    model.add(BatchNormalization())


    #### conv8
    # conv8_1
    model.add(ZeroPadding2D(padding = (1, 1)))
    model.add(Convolution2D(256, (4, 4), strides = (2, 2)))
    model.add(Activation('relu'))

    # conv8_2
    model.add(ZeroPadding2D(padding = (1, 1)))
    model.add(Convolution2D(256, (3, 3)))
    model.add(Activation('relu'))

    # conv8_3
    model.add(ZeroPadding2D(padding = (1, 1)))
    model.add(Convolution2D(256, (3, 3)))
    model.add(Activation('relu'))


    #### Softmax
    model.add(Convolution2D(484, (1, 1)))
    model.add(Activation(softmax3))

    ## Loss
    model.compile(loss = custom_loss_rebalancing_segments, optimizer = Adam(decay = 0.001))

    return model


if __name__ == '__main__':
    model = build_model()
    print model.output_shape
